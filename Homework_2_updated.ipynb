{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group: XX (Name 1, Name 2, Name 3, Name 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "The purpose of this homework is to get acquainted with the solution of linear systems using elementary methods. As a model problem, we will work with polynomial interpolation. We will also use this homework as an opportunity to examine and familiarize ourselves with the Numpy library for Python, which makes it extremely easy to work with arrays in general, and vectors and matrices in particular.\n",
    "\n",
    "- - -\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### NumPy and Linear Algebra in Python\n",
    "</div>\n",
    "\n",
    "There are several excellent sources of documentation for Numpy.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Task 1:** Familiarize yourself with Numpy by reading the [quickstart tutorial](https://docs.scipy.org/doc/numpy/user/quickstart.html):\n",
    "- In particular, since we will be working with matrices (two-dimensional arrays), make sure you understand indexing and slicing.\n",
    "- Also, be aware of the semantics of assignments (view vs. copy), which is sometimes tricky for beginners. More [here](https://www.jessicayung.com/numpy-views-vs-copies-avoiding-costly-mistakes/) and [here](https://scipy-cookbook.readthedocs.io/items/ViewsVsCopies.html).\n",
    "</div>\n",
    "\n",
    "Let's get started by importing the relevant packages and doing some basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              # basic arrays, vectors, matrices\n",
    "import numpy.linalg as la       # linear algebra functions\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Polynomial Interpolation\n",
    "</div>\n",
    "\n",
    "*Interpolation* refers to the process of extrapolating from discrete measurements to a continuous function. A model problem is for example to compute the continuous trajectory $x(t)$ of an object over time from discrete snapshots $(t_i,x_i)$. *Polynomial interpolation* specifically refers to describing the continuous function as a polynomial.\n",
    "\n",
    "Formally, the (one-dimensional) polynomial interpolation problem is described as follows: given a sequence of $n$ $x$-coordinates $(x_0,\\dots,x_n)$ with $x_i \\neq x_j$ if $i\\neq j$ and corresponding values $y_i \\in \\mathbb{R}$, determine a polynomial $p(x)$ of degree $n$ such that\n",
    "\n",
    "$$\n",
    "p(x_i)\\ =\\ a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_1 x + a_0\\ \\stackrel{!}{=}\\ y_i,\\ \\ i=0,\\dots,n.\n",
    "$$\n",
    "\n",
    "This problem can be turned into a linear system, as follows:\n",
    "\n",
    "Let $v_j$ the vector of powers of $v_j$, i.e. \n",
    "\n",
    "$$v_j = (1, x_j, x_j^2, \\dots, x_j^n)^T,$$ \n",
    "\n",
    "and let $a = (a_0, a_1, \\dots, a_n)^T$ the vector of coefficients of $p$. Then, the interpolation condition $p(x_i) = y_i$ can be written as the scalar product of $q_i$ and $a$, since\n",
    "\n",
    "$$\n",
    "p(x_i)\\ =\\ a_n x_i^n + \\cdots + a_1 x_i + 1 \\cdot a_0 = \\langle a, v_i \\rangle\\ =\\ a^T v_i\\ =\\ \\stackrel{!}{=} y_i\n",
    "$$\n",
    "\n",
    "Repeating this for $i=0,\\dots,n$, we obtain a linear system for the coefficients:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & x_0 & x_0^2 & \\cdots & x_0^n \\\\\n",
    "1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\\n",
    "1 & x_2 & x_2^2 & \\cdots & x_2^n \\\\\n",
    "\\vdots & \\vdots &\\vdots &\\vdots & \\\\\n",
    "1 & x_n & x_n^2 & \\cdots & x_n^n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n\\end{pmatrix} =\n",
    "\\begin{pmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\\end{pmatrix}\n",
    "\\qquad\n",
    "$$\n",
    "\n",
    "or in brief\n",
    "\n",
    "$$V a = y$$\n",
    "\n",
    "The $(n+1)\\times (n+1)$ system matrix $V$ with $V_{ij} = x_i^j$ is called a *Vandermonde matrix*. A solution of the corresponding system gives the coefficients $a$ of the polynomial $p$.\n",
    "\n",
    "Let's consider a specific problem instance: first, define NumPy arrays containing the data $x_i$ and $y_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "y = np.array( [3., 5, 2, 3, 1] )\n",
    "x = np.array( [0., 1, 2, 3, 4] )\n",
    "\n",
    "plt.plot( x, y, 'bo' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, our goal is to construct a polynomial of degree $4$ (since there are five points) that exactly passes through these points. \n",
    "\n",
    "(You have encountered this problem previously when constructing a line (= polynomial of degree one) given two points, or a parabola (= polynomial of degree two) that passes through three given points.\n",
    "\n",
    "We begin by constructing the Vandermonde system matrix for the $x_i$. Naively, we would write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.empty( (n+1, n+1) )\n",
    "\n",
    "# directly fill in matrix entries\n",
    "for i in range(n+1):\n",
    "    for j in range(n+1):\n",
    "        V[i,j] = x[i]**j\n",
    "        \n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can write this a little more compactly using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2D array whose rows are the powers of x,\n",
    "# then transpose so the columns are the powers of x\n",
    "V = np.array( [x**p for p in range(n+1)] )\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even shorter: NumPy directly offers the [`vander`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vander.html) function for constructing $V$. By default, `vander` constructs the matrix such that the powers decrease along the rows of $V$; we thus need to pass `increasing=True` to match the order of $a$ we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V = np.vander( x, increasing=True )\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can solve the system $Va = y$ for $a$ to obtain the coefficients of the polynomial $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Gauss Elimination\n",
    "</div>\n",
    "\n",
    "As discussed in the lecture, the Gauss elimination algorithm can be used to solve the system above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Task 2:** Implement the Gauss algorithm (without pivoting) in two steps through the below functions:\n",
    "- `bwd_subs` should perform backward substitution to solve an upper tridiagonal system\n",
    "- `gauss_solve` should reduce the extended matrix $(A,b)$ to upper tridiagonal form via Gauss elimination, and then use `bwd_subs` to solve the system\n",
    "- It is always sensible to think about ways to validate your implementations, e.g. check that `bwd_subs` does the right thing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bwd_subs( U, y ):\n",
    "    \"\"\"Solve the linear system Ux = y with upper triangular matrix U by forward substitution.\"\"\"\n",
    "    n = U.shape[0]\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    return x\n",
    "\n",
    "def gauss_solve( A, b ):\n",
    "    \"\"\"Solve the linear system Ax=b using direct Gaussian elimination and backward substitution.\"\"\"\n",
    "   \n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # TODO\n",
    "            \n",
    "    return np.zeros(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to our problem and validate the solution graphically. NumPy conveniently provides the [`polyval`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html) function to evaluate the found polynomial. \n",
    "\n",
    "A potential showstopper is that `polyval` requires as input the coefficients in descending order (i.e. `a[0]` as the coefficient of $x^n$), while we compute them in ascending order (`a[n]` is the coefficient of $x^n$). This is however easily remedied using NumPy's [`flip`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html?highlight=flip#numpy.flip) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = gauss_solve( V, y )\n",
    "\n",
    "# plot output\n",
    "xx = np.linspace( x.min(), x.max(), 100 )\n",
    "yy = np.polyval( np.flip(a), xx )\n",
    "\n",
    "plt.plot( xx, yy, 'r-', x, y, 'bo' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem solved! (... if you completed task 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### LU Decomposition\n",
    "</div>\n",
    "\n",
    "While Gauss elimination works well for a small number of points, for a large number of points it does not work well. Let's look at a larger problem instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace( -1, 1, n )\n",
    "y = np.cos( 15*x**2 )\n",
    "\n",
    "# set up system matrix\n",
    "V = np.vander( x, increasing=True )\n",
    "a = gauss_solve( V, y )\n",
    "\n",
    "# plot output\n",
    "xx = np.linspace( x.min(), x.max(), 100 )\n",
    "yy = np.polyval( np.flip(a), xx )\n",
    "\n",
    "plt.plot( xx, yy, 'r-', x, y, 'bo' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look entirely right. In order to quantify the error, we can compute the norm of the *residual* \n",
    "\n",
    "$$r := Va - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.norm( np.matmul(V, a) - y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see that it is not actually small. \n",
    "\n",
    "The reason that the residual is so large lies in the ill-conditioned nature of the system matrix. NumPy gives us an easy way to compute the condition number of the system matrix, which indicates how much small errors are amplified when solving the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.cond( V )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a big number. While the Gauss algorithm works well for well-conditioned problems, it is not stable enough to address severely ill-conditioned problems; hence it does not work well here.\n",
    "\n",
    "**Aside**: It is easy to see that the condition number increases rapidly with $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( [np.log10(la.cond(np.vander(np.linspace(0,1,n)))) for n in range(1,20)] );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of points increases, the columns of the Vandermonde matrix become more and more similar and thus less and less linear independent; for 10 points, the condition number is already $\\approx 10^8$. In a sense, the system becomes more and more difficult to solve, leading to increasing problems in the application of numerical techniques. (This is quite typical behavior for many important numerical problems / techniques.)\n",
    "\n",
    "To remedy this, let's apply the LU decomposition with partial pivoting from the course to solve this system. The purpose of the pivoting is to increase the numerical stability of the algorithm.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 3**: implement routines `lu_factor` and `fwd_subs` below for computing and using the LU(P) decomposition\n",
    "\n",
    "- The algorithm for `lu_factor` is given on slide 2-40 in the course slides. Note that while indices of matrix rows etc. start at $1$ in standard mathematical notation (and hence in the slides), NumPy indices begin a $0$.\n",
    "- `fwd_subs` should perform forward substitution, similar to `bwd_subs` above.\n",
    "- `lu_solve` is already completely implemented.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_factor(A):\n",
    "    \"\"\"Perform an LU decomposition PA=LU with partial pivoting of the square matrix A.\"\"\"\n",
    "\n",
    "    n = A.shape[0]\n",
    "\n",
    "    P = np.identity(n)\n",
    "    U = np.identity(n)\n",
    "    L = np.identity(n)\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return (P, L, U)\n",
    "\n",
    "\n",
    "def fwd_subs(L, b):\n",
    "    \"\"\"Solve the linear system Ly = b with lower triangular matrix L by forward substitution.\"\"\"\n",
    "    y = np.empty_like(b)\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def lu_solve(P, L, U, b):\n",
    "    \"\"\"Solve the linear system LUx = Pb with upper/lower triangular matrices U, L and\n",
    "       permutation matrix P.\"\"\"\n",
    "    \n",
    "    # make sure all matrices are of the required form\n",
    "    assert np.all(U == np.triu(U)) # ensure U is upper triangular\n",
    "    assert np.all(L == np.tril(L)) # ensure L is upper triangular\n",
    "    assert np.all(np.logical_or(P == 0, P == 1)) and np.count_nonzero(P) == n\n",
    "\n",
    "    return bwd_subs(U, fwd_subs(L, np.matmul(P, b)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to our problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, L, U = lu_factor(V)\n",
    "a = lu_solve(P, L, U, y)\n",
    "\n",
    "print( \"residual = \", la.norm( np.matmul(V, a)-y ) )\n",
    "\n",
    "# plot output\n",
    "xx = np.linspace( x.min(), x.max(), 200 )\n",
    "yy = np.polyval( np.flip(a), xx )\n",
    "\n",
    "plt.plot( xx, yy, 'r-', x, y, 'bo' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Microbenchmarks\n",
    "</div>\n",
    "\n",
    "Beyond numerical stability from pivoting, the LU decomposition also has one other advantage: the elimination work is captured in $L$ and $U$ and does not have to be repeated if the same linear system must be solved repeatedly for different right-hand sides.\n",
    "\n",
    "This is a good opportunity to use a *microbenchmark* to quickly get an idea whether this is true in practice. Let's create one using Jupyter's [`%%timeit`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) magic that measures the runtime of a single cell. First, set up a test problem, using again polynomial interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "x = np.linspace( 0, 1, n )\n",
    "\n",
    "# set up the system matrix\n",
    "V = np.vander( x, increasing=True )\n",
    "\n",
    "# create m random right-hand sides\n",
    "m = 500\n",
    "y = np.random.random(size=(m,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can measure the time required to do one decomposition and $m$ solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "P, L, U = lu_factor(V)\n",
    "\n",
    "for b in y:\n",
    "    a = lu_solve(P, L, U, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 4**: create a microbenchmark that uses `gauss_solve` to solve the system for each right-hand side.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your implementation, the LU decomposition should be significantly faster on this particular problem.\n",
    "\n",
    "Note: this is not a rigorous benchmark; it may be influenced by many factors such as overall system load etc. However, `%%timeit` can be useful to get a quick idea of execution time. (See the [documentation](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) for more info.)\n",
    "\n",
    "**Concluding remark**: While it was instructive for the purposes of this homework to implement Gauss elimination and LU decomposition by ourselves, for real-world use, NumPy's linear algebra module offers the routine [`numpy.linalg.solve`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html), which is a state-of-the-art linear system solver with many tricks to improve numerical stability. Furthermore, the `scipy` module contains implementations of the LU(P) decomposition (cf. [`scipy.linalg.lu`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.lu.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Polynomial Approximation\n",
    "</div>\n",
    "\n",
    "As we saw above, polynomially interpolation of $n$ points gives a polynomial for degree $n+1$. For even moderately large $n$ ($n \\gg 5$), this is numerically unpleasant. Furthermore, these polynomials tend to oscillate strongly, limiting their usefulness in practice.\n",
    "\n",
    "While there are other interpolation techniques that do not suffer from these problems (e.g. [Splines](https://en.wikipedia.org/wiki/Spline_(mathematics))), in many application, it is sufficient to *approximate* a curve that fits a given set of points. (This is sometimes also called *regression*.)\n",
    "\n",
    "Formally, given $n$ points and seeking a polynomial of degree $m$, we can formulate this *approximation problem* as an *overdetermined* linear system (using the same notation as above):\n",
    "\n",
    "$$Va = y$$\n",
    "\n",
    "with $V \\in \\mathbb{R}^{n\\times(m+1)}$, $y \\in \\mathbb{R}^n$, and $a \\in \\mathbb{R}^{m+1}$, for which we seek a to find a *least-squares solution* by minimizing the squared residual:\n",
    "\n",
    "$$ r^2 = \\| y-Va \\|_2^2 $$\n",
    "\n",
    "As discussed in the course, this leads to the *normal equations*\n",
    "\n",
    "$$ V^T V a = V^T x.$$\n",
    "\n",
    "Since $V^T V$ is a square matrix, this can be solved using the techniques we investigated above.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 5**: Implement below the function `poly_approx` that computes an approximating polynomial of degree $m$ for input $x$ and $y$ as above.\n",
    "- You can use `np.vander` and discard the columns $m+1,\\dots,n$ to compute $V$, or compute it as above.\n",
    "- Feel free to use any linear solver you want (e.g. your own or `numpy.linalg.solve`, `numpy.linalg.cholesky`, or (probably best if already discussed in the course) `numpy.linalg.qr` directly on $V$).\n",
    "- Compute and return from `poly_approx` the squared residual $r^2 := \\|y - Va\\|_2^2$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_approx( x, y, m ):\n",
    "    \"\"\"Compute an approximation polynomial of degree m for the points (x,y)\"\"\"\n",
    "    \n",
    "    s = np.zeros( x.shape )\n",
    "    r2 = 0\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return s, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's graphically validate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set up problem   \n",
    "x = np.linspace(0, 1, 150)\n",
    "y = np.random.random(150) + np.cos( 12*x )\n",
    "\n",
    "# approximate\n",
    "a, r2 = poly_approx( x, y, 4 )\n",
    "print( \"r2 =\", r2 )\n",
    "\n",
    "# plot output\n",
    "xx = np.linspace( x.min(), x.max(), 200 )\n",
    "yy = np.polyval( np.flip(a), xx )\n",
    "\n",
    "plt.plot( xx, yy, 'r-', x, y, 'b.' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the polynomial looks only loosely like the data. By increasing the degree, we can also improve the fit (as given by $r^2$).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 6**: Write code to determine the minimum degree for which the fit is good enough, by using $r^2 < 20$ as an acceptance criterion. Plot the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Going to Space\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used polynomial interpolation/approximation only for one-dimensional function graphs of the form $y=p(x)$. However, the basic methodology carries easily to interpolating a curve in $n$ dimensions (e.g. in two dimensions).\n",
    "\n",
    "We start from a modified basic setup: given $t_0,\\dots,t_n$ and $x_0,\\dots,x_n \\in \\mathbb{R}^d$, find the vector-valued polyomial $p(x)$ of degree $n+1$ such that \n",
    "\n",
    "$$p(t_i) = x_i.$$\n",
    "\n",
    "It can be easily recognizes that this can be solved by finding $d$ separate one-dimensional (i.e. real-valued) polynomials $p_1,\\dots,p_d$, each of degree $n+1$, for each of the $d$ components of the $x_i$. \n",
    "\n",
    "The following routine accomplishes this in the two-dimensional case, where `x` and `y` are arrays of coordinates of the interpolation points in the plane, and `t` is the array of the $t_i$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2D(t, x, y):\n",
    "    return np.linalg.solve( \n",
    "        np.vander( t, increasing=True ), \n",
    "        np.stack([x,y]).transpose() \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this to the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "t = np.linspace(0, 12, n)\n",
    "x = (3+t) * np.cos( t )\n",
    "y = (3+t) * np.sin( t )\n",
    "\n",
    "a = poly2D( t, x, y )\n",
    "\n",
    "# plot output\n",
    "tt = np.linspace( t.min(), t.max(), 200 )\n",
    "xx = np.polyval( np.flip(a[:,0]), tt )\n",
    "yy = np.polyval( np.flip(a[:,1]), tt )\n",
    "\n",
    "plt.plot( x, y, 'bo', xx, yy, 'r-' )\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 7**: Implement a function `poly3D`, by generalizing `poly2D` above to the three-dimensional case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly3D(t, x, y, z):\n",
    "    # TODO\n",
    "    return np.empty_like(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Task 8**: Illustrate the correctness of `poly3D` on a model problem similar to the one for `poly2D` above.\n",
    "\n",
    "Notes: \n",
    "- To show the curve, you will have to use matplotlib's 3D plotting module. Documentation can be found [here](https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html), and a short tutorial is [here](https://towardsdatascience.com/an-easy-introduction-to-3d-plotting-with-matplotlib-801561999725).\n",
    "- Using matplotlib's notebook mode (triggered by `%matplotlib notebook`) will give you a nice, interactively rotatable plot. Try it out!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Hermite Interpolation (Change of Basis)\n",
    "</div>\n",
    "\n",
    "A fundamental reason that polynomial interpolation can be interpreted as a linear system is that the space of polynomials (of a fixed degree $\\leq n$) is a linear vector space of finite dimension.\n",
    "\n",
    "We usually write polynomials of degree $n$ as\n",
    "\n",
    "$$\n",
    "p(x) = a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_1 x + a_0.\n",
    "$$\n",
    "\n",
    "In other words, every polynomial is represented by a (unique) linear combination of $n+1$ basis functions $x^i$ (the so-called *monomials*) with coefficients $a_0,\\dots,a_n$. One easily finds that all other requirements for a vector space over $\\mathbb{R}$ are also fulfilled: we can easily add, subtract, and form scalar multiples of polynomials which does not change the degree, so the space is closed under these operations. Similarly, all other properties are easily validated.\n",
    "\n",
    "Using this interpretation, we can, without problems, find other bases that make it easier to solve specific problems.\n",
    "\n",
    "For example, in many applications it is necessary to find polynomial that interpolate, but also fulfill requirements on their derivative. Coinsider the *cubic Hermite polynomials*\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "H_0(x) &=& 2x^3 - 3x^2 + 1\\\\\n",
    "H_1(x) &=& -2x^3 + 3x^2 \\\\\n",
    "H_2(x) &=& x^3 - 2x^2 + x\\\\\n",
    "H_3(x) &=& x^3 - x^2\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "h0 = [1, 0, -3,  2]\n",
    "h1 = [0, 0,  3, -2]\n",
    "h2 = [0, 1, -2,  1]\n",
    "h3 = [0, 0, -1,  1]\n",
    "\n",
    "x = np.linspace( 0, 1 )\n",
    "y = [np.polyval( np.flip(h), x) for h in [h0, h1, h2, h3]]\n",
    "\n",
    "plt.plot( x, y[0], x, y[1], x, y[2], x, y[3] );\n",
    "plt.legend(['h0','h1','h2','h3']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting properties of the cubic Hermite polynomials are the following:\n",
    "\n",
    "- At $x=0$, all $H_i$ are zero except $H_0$, which is one.\n",
    "- At $x=1$, all $H_i$ are zero except $H_1$, which is one.\n",
    "- At $x=0$, all $H_i$ have zero derivative, except $H_2$, which has derivative H_2'(0) = 1.\n",
    "- At $x=1$, all $H_i$ have zero derivative, except $H_3$, which has derivative H_3'(1) = 1.\n",
    "\n",
    "This can be seen by simply plugging $x=0$ and $x=1$ into the $H_i$ and $H_i'$.\n",
    "\n",
    "Furthermore, the $H_i$ span the space of cubic polynomials, since the matrix of their coefficients in the monomial basis has full rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H = np.array( [h0, h1, h2, h3] )\n",
    "\n",
    "print(H)\n",
    "print(\"rank =\", la.matrix_rank(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the $H_i$ also form a basis of the space of cubic polynomials, and every cubic polynomial can be written as\n",
    "\n",
    "$$p(x) = c_0 H_0(x) + c_1 H_1(x) + c_2 H_2(x) + c_3 H_3(x).$$\n",
    "\n",
    "The significance of these polynomials lies in the fact that it is possible to directly solve the following interpolation problem: given $x_0, x_1, d_0, d_1$, find a cubic polynomial $q(x)$ such that \n",
    "\n",
    "$$q(0) = x_0,\\ q(1) = x_1,\\ q'(0) = d_0,\\ q'(1) = d_1.$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Bonus Task 9**: Compute the cubic polynomial as above for $x_0 = 10, x_1 = 2, d_0 = -3, d_1 = 3$.\n",
    "\n",
    "Hint: you do not (!!!) need to solve a linear system -- that is the appeal of the Hermite basis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
